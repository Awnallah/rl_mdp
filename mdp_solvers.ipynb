{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "from newFrozen import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0, eps_v = 1e-10):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    P = env.P\n",
    "    v = np.zeros(num_states)\n",
    "    diff_vs = []\n",
    "    v_sum = []\n",
    "    iters = 0\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        v_old = np.copy(v)\n",
    "        Q = np.zeros((num_states, num_actions))\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                for prob, s_next, reward, done in P[s][a]:\n",
    "                    if not done:\n",
    "                        Q[s][a] += prob * (reward + (gamma* v_old[s_next]))\n",
    "                    else:\n",
    "                        Q[s][a] += prob * (reward)\n",
    "        v = np.max(Q,1)\n",
    "        diff = np.max(np.abs(v - v_old))\n",
    "        diff_vs.append(diff)\n",
    "        v_sum.append(v.sum())\n",
    "        if(diff < eps_v):\n",
    "            converged = True\n",
    "        iters += 1\n",
    "    return v, iters, v_sum , diff_vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(env,v, gamma = 1.0):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    P = env.P\n",
    "    policy = np.zeros(num_states)\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            for prob, s_next, r, done in P[s][a]:\n",
    "                if not done:\n",
    "                    action_values[a] += (prob * (r + gamma * v[s_next]))\n",
    "                else:\n",
    "                    action_values[a] += (prob * (r))\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    return policy\n",
    "\n",
    "def runPolicy(env, policy, useDiscount = True, gamma = 1.0):\n",
    "    reward = 0\n",
    "    s = env.reset()\n",
    "    i = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, _ = env.step(policy[s])\n",
    "        if useDiscount:\n",
    "            reward += (gamma**i * r)\n",
    "        else:\n",
    "            reward += r\n",
    "        i += 1\n",
    "    return reward\n",
    "\n",
    "def evaluate_policy(env, policy, useDiscount = True, gamma = 1.0,  nSamples = 100):\n",
    "    return np.mean([runPolicy(env, policy, useDiscount, gamma) for _ in range(nSamples)])\n",
    "\n",
    "def compute_policy_v(env, policy, gamma = 1.0, eps_v = 1e-10, maxIters = 2000):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    P = env.P\n",
    "    v = np.zeros(num_states)\n",
    "    iters = 0\n",
    "    converged = False\n",
    "    while not converged and iters < maxIters:\n",
    "        v_prev = np.copy(v)\n",
    "        for s in range(num_states):\n",
    "            a = policy[s]\n",
    "            action_value = 0\n",
    "            for prob, s_next, r, done in P[s][a]:\n",
    "                if not done:\n",
    "                    action_value += prob * (r + gamma * v_prev[s_next])\n",
    "                else:\n",
    "                    action_value += prob * (r)\n",
    "            v[s] = action_value\n",
    "        if(np.max(np.abs(v - v_prev)) < eps_v):\n",
    "            converged = True\n",
    "        iters += 1\n",
    "    return v, iters\n",
    "\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0, maxIters = 200000,  eps_v = 1e-10, nSamples = 100):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    v_values = []\n",
    "    n_iters = []\n",
    "    P = env.P\n",
    "    converged = False\n",
    "    policy = np.random.choice(num_actions, num_states)\n",
    "    i = 0\n",
    "    while i < maxIters and not converged:\n",
    "        policy_prev = np.copy(policy)\n",
    "\n",
    "        v, iters = compute_policy_v(env, policy, gamma, eps_v)\n",
    "        n_iters.append(iters)\n",
    "\n",
    "        v_values.append(v.sum())\n",
    "\n",
    "        policy = extract_policy(env, v, gamma)\n",
    "\n",
    "        if(np.all(policy == policy_prev)):\n",
    "            converged = True\n",
    "        i += 1\n",
    "    return policy, i, v_values, n_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(problem, maxIters = 200, gamma = 1.0, alpha = .01, episodes = 10000, epsilon = 1,\n",
    " epsilon_min = .1, epsilon_decay = .9):\n",
    "    nState = problem.observation_space.n\n",
    "    nAction = problem.action_space.n\n",
    "    Q = np.random.rand(nState, nAction)\n",
    "    q_scores = []\n",
    "    q_iters_s = []\n",
    "    q_eps = []\n",
    "    cnt = Counter()\n",
    "    for i in range(episodes):\n",
    "        s = problem.reset()\n",
    "        done = False\n",
    "        Q_old = Q.copy()\n",
    "        step = 0\n",
    "        while not done:\n",
    "            a = problem.action_space.sample()\n",
    "            if random.uniform(0,1) > epsilon:            \n",
    "                a = np.argmax(Q[s])\n",
    "            s_, r, done, info = problem.step(a)\n",
    "            q_old = Q[s,a]\n",
    "            q_new = (1-alpha)*q_old + alpha*(r+gamma*np.max(Q[s_,:]))\n",
    "            Q[s,a] = q_new\n",
    "            s = s_\n",
    "            step += 1\n",
    "        epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "        q_eps.append(epsilon)\n",
    "\n",
    "        pol = np.argmax(Q, 1)\n",
    "        cnt[str(pol)] += 1\n",
    "        \n",
    "        \n",
    "        if(i%100 == 99):\n",
    "\n",
    "            # most_common = cnt.most_common(3)\n",
    "            # spol = reduce(lambda x,y: str(x)+str(y), pol)\n",
    "            score = evaluate_policy(problem, pol)\n",
    "            q_scores.append(score)\n",
    "            q_iters_s.append(i)\n",
    "            \n",
    "            # print(\"iteration: {}\\t epsilon: {}\\t policy: {}\\t score: {} \\t most common: {}\".format(i,epsilon,spol,score,[x[1] for x in most_common]))\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    # plt.savefig(title + \"/Iterations per policy\")\n",
    "    # plt.close()\n",
    "    return np.argmax(Q, 1), episodes, q_iters_s ,q_scores, q_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_4 = FrozenLakeEnv(map_name=\"4x4\")\n",
    "frozen_20 = FrozenLakeEnv(map_name=\"20x20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_envs = [frozen_4]\n",
    "decay = 0.99\n",
    "title = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem in fl_envs:\n",
    "    if hasattr(problem, 'env'):\n",
    "        env = problem.env\n",
    "    else:\n",
    "        env = problem\n",
    "    print(\"XXXXXX\" , str(env), \"XXXXXX\")\n",
    "    print(\"states: {}\\n actions: {}\".format(env.nS, env.nA))\n",
    "\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    v_value, i_value, summed, diff = value_iteration(env, 1.0)\n",
    "\n",
    "    plt.plot(diff)\n",
    "    plt.title(title + \" Diff in summed V- VI\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Diff is summed V\")\n",
    "#     plt.savefig( title + \"/v_diff\")\n",
    "#     plt.close()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    plt.plot(summed)\n",
    "    plt.title(title + \" state Values -VI\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"sum of values per state\")\n",
    "#     plt.savefig(title + \"/v_summed\")\n",
    "#     plt.close()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    tf = time.perf_counter()\n",
    "    t_value = tf-t0\n",
    "    # get policy\n",
    "    p_star_value = extract_policy(env, v_value)\n",
    "    score_value = evaluate_policy(env, p_star_value, False)\n",
    "    #####\n",
    "    print(\"VI:\\n\\ttime: {}\\n\\titers: {}\\n\\tscore: {}\".format(t_value, i_value,score_value))\n",
    "    ###########\n",
    "\n",
    "\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    p_star_policy, i_policy, p_values, n_iters = policy_iteration(env)\n",
    "    # print(p_star_policy)\n",
    "    plt.plot(range(1,i_policy+1),p_values)\n",
    "    print(\"highest PI V: \", p_values[-1])\n",
    "    plt.title(title + \" Policy values - PI\")\n",
    "    plt.xlabel(\"policiy number\")\n",
    "    plt.ylabel(\"summed value for policy\")\n",
    "#     plt.savefig(title + \"/P_summed\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(range(1,i_policy+1),n_iters)\n",
    "    # print(n_iters)\n",
    "    plt.title(title + \" iterations per V of Policy - PI\")\n",
    "    plt.xlabel(\"policiy number\")\n",
    "    plt.ylabel(\"# of Iterations\")\n",
    "#     plt.savefig(title + \"/Iterations per policy\")\n",
    "#     plt.close()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    tf = time.perf_counter()\n",
    "    t_policy = tf-t0\n",
    "    # derive value matrix\n",
    "    v_policy = compute_policy_v(env, p_star_policy)\n",
    "    # v_policys = [compute_policy_v(env, i) for  ]\n",
    "    score_policy = evaluate_policy(env, p_star_policy, False)\n",
    "    print(\"Policy Iteration:\\n\\ttime: {}\\n\\titers: {}\\n\\tscore: {}\".format(t_policy, i_policy,score_policy))\n",
    "    #####    \n",
    "\n",
    "    print(\"I policy the same: {}\".format(np.all(p_star_value == p_star_policy)))\n",
    "\n",
    "\n",
    "\n",
    "        # do Q-learning\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    p_star_Q, i_Q, q_iters_s, q_scores_iter, q_eps = QLearning(problem, epsilon_decay = decay)\n",
    "    tf = time.perf_counter()\n",
    "    t_Q = tf-t0\n",
    "    score_Q = evaluate_policy(env, p_star_Q, False)\n",
    "\n",
    "    plt.plot(q_iters_s,q_scores_iter)\n",
    "    # print(n_iters)\n",
    "    plt.title(title + \"Qlearning scores vs Iterations. decay factor \" + str(decay))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"scores\")\n",
    "    name = title + \"/\" + str(decay) +\" Qlearning scores.png\"\n",
    "#     plt.savefig(name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(q_eps)\n",
    "    # print(n_iters)\n",
    "    plt.title(title + \"QL Decay: epsilon vs Iterations. decay factor \" + str(decay))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"epsilon\")\n",
    "    name1 = title + \"/\" + str(decay) +\" epsilon_decay.png\"\n",
    "#     plt.savefig(name1)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    print(\"QL:\\n\\ttime: {}\\n\\titers: {}\\n\\tscore: {}\".format(t_Q, i_Q,score_Q))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
